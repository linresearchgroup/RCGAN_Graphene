{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import relevant packages and fix the seeds for repeatability\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import pickle\n",
    "import keras\n",
    "import os, time\n",
    "from keras import backend as K\n",
    "from keras.utils import np_utils\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import partial\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.2)\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "np.random.seed(int(time.time()))\n",
    "random.seed(1)\n",
    "session_conf = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1, gpu_options = gpu_options)\n",
    "tf.set_random_seed(1)\n",
    "sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n",
    "K.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the pre-processed data file and format it to satisfy the neural network\n",
    "\n",
    "with open('train.pickle', 'rb') as f:\n",
    "    X_train, Y_train = pickle.load(f)\n",
    "    \n",
    "X_train = X_train.reshape([X_train.shape[0], 4, 4, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out outliers using inter-quartile range\n",
    "IQR = - np.quantile(Y_train, 0.25) + np.quantile(Y_train, 0.75)\n",
    "\n",
    "lower_bound, upper_bound = np.quantile(Y_train, 0.25) - 1.5 * IQR, np.quantile(Y_train, 0.75) + 1.5 * IQR\n",
    "\n",
    "idx, val = np.where((Y_train >= lower_bound) & (Y_train <= upper_bound))\n",
    "\n",
    "Y_train = Y_train[idx]\n",
    "X_train = X_train[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check on the data shape and range of bandgaps\n",
    "X_train.shape, Y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.min(Y_train), np.max(Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Here are one sample of the data, X_train[0] is a material structure while Y_train[0] is its corresponding bandgap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[-1.],\n",
       "        [-1.],\n",
       "        [-1.],\n",
       "        [-1.]],\n",
       "\n",
       "       [[-1.],\n",
       "        [-1.],\n",
       "        [-1.],\n",
       "        [-1.]],\n",
       "\n",
       "       [[-1.],\n",
       "        [-1.],\n",
       "        [-1.],\n",
       "        [-1.]],\n",
       "\n",
       "       [[-1.],\n",
       "        [ 1.],\n",
       "        [ 1.],\n",
       "        [ 1.]]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.7233])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create placeholders\n",
    "real_data = tf.placeholder(tf.float32, shape = [None, 4, 4, 1])\n",
    "z = tf.placeholder(tf.float32, shape = [None, 128])\n",
    "y = tf.placeholder(tf.float32, [None, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leaky Relu\n",
    "def lrelu(X, leak = 0.2):\n",
    "    f1 = 0.5 * (1 + leak)\n",
    "    f2 = 0.5 * (1 - leak)\n",
    "    return f1 * X + f2 * tf.abs(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Structure of the generator\n",
    "def generator(z, y_, reuse=None):\n",
    "    with tf.variable_scope('G',reuse=reuse):      \n",
    "        \n",
    "        y_ = tf.concat([z, y_], axis = 1)\n",
    "        \n",
    "        hidden2 = tf.layers.dense(y_, units = 1 * 1 * 512)\n",
    "        R1 = tf.reshape(hidden2, shape = [-1,1,1,512])\n",
    "        \n",
    "        for i in range(5):\n",
    "            C1 = tf.layers.conv2d_transpose(R1, filters = 512, kernel_size = 3,\n",
    "                                            strides = (1, 1), padding = 'same')\n",
    "            B1 = tf.layers.batch_normalization(C1)\n",
    "            R1 = lrelu(B1)\n",
    "        \n",
    "        C1 = tf.layers.conv2d_transpose(R1, filters = 256, kernel_size = 3,\n",
    "                                        strides = (2, 2), padding = 'same')\n",
    "        B1 = tf.layers.batch_normalization(C1)\n",
    "        R1 = lrelu(B1)\n",
    "        \n",
    "        C2 = tf.layers.conv2d_transpose(R1, filters = 128, kernel_size = 3,\n",
    "                                       strides = (2, 2), padding = 'same')\n",
    "        B2 = tf.layers.batch_normalization(C2)\n",
    "        R2 = lrelu(B2)\n",
    "        \n",
    "        C3 = tf.layers.conv2d_transpose(R2, filters = 1, kernel_size = 3,\n",
    "                                       strides = (1, 1), padding = 'same')\n",
    "        O = tf.nn.tanh(C3)\n",
    "        \n",
    "        return O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Structure of the discriminator\n",
    "def discriminator(X, y_, reuse=None):\n",
    "    with tf.variable_scope('D',reuse=reuse):\n",
    "  \n",
    "        y1 = tf.concat([X, y_], axis = 1)\n",
    "        \n",
    "        y2 = tf.layers.dense(y1, units = 256)\n",
    "        y2 = lrelu(y2)\n",
    "        y2 = tf.layers.dropout(y2, 0.5)\n",
    "        \n",
    "        yout = tf.layers.dense(y2, units = 1)\n",
    "        O = tf.nn.sigmoid(yout)\n",
    "        \n",
    "        return O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Structure of regressor\n",
    "def regressor(X, reuse = None):\n",
    "    with tf.variable_scope('R', reuse = reuse):\n",
    "        tower0 = tf.layers.conv2d(X, filters = 32, kernel_size = (1, 1),\n",
    "                                 padding = 'same')\n",
    "        \n",
    "        tower1 = tf.layers.conv2d(X, filters = 64, kernel_size = (1, 1),\n",
    "                                 padding = 'same')\n",
    "        tower1 = tf.layers.conv2d(tower1, filters = 64, kernel_size = (3, 3),\n",
    "                                 padding = 'same')\n",
    "        \n",
    "        tower2 = tf.layers.conv2d(X, filters = 32, kernel_size = (1, 1),\n",
    "                                 padding = 'same')\n",
    "        tower2 = tf.layers.conv2d(tower2, filters = 32, kernel_size = (5, 5),\n",
    "                                  padding = 'same')\n",
    "        \n",
    "        tower3 = tf.layers.max_pooling2d(X, pool_size = (3, 3), strides = (1, 1),\n",
    "                                        padding = 'same')\n",
    "        tower3 = tf.layers.conv2d(tower3, filters = 32, kernel_size = (1, 1),\n",
    "                                 padding = 'same')\n",
    "        \n",
    "        h = tf.concat([tower0, tower1, tower2, tower3], axis = -1)\n",
    "        h = tf.nn.relu(h)\n",
    "        h = tf.layers.max_pooling2d(h, pool_size = (2, 2), strides = (1, 1), padding = 'same')\n",
    "      \n",
    "        for i in range(6):\n",
    "            tower0 = tf.layers.conv2d(h, filters = 32, kernel_size = (1, 1),\n",
    "                                 padding = 'same')\n",
    "        \n",
    "            tower1 = tf.layers.conv2d(h, filters = 64, kernel_size = (1, 1),\n",
    "                                     padding = 'same')\n",
    "            tower1 = tf.layers.conv2d(tower1, filters = 64, kernel_size = (3, 3),\n",
    "                                     padding = 'same')\n",
    "\n",
    "            tower2 = tf.layers.conv2d(h, filters = 32, kernel_size = (1, 1),\n",
    "                                     padding = 'same')\n",
    "            tower2 = tf.layers.conv2d(tower2, filters = 32, kernel_size = (5, 5),\n",
    "                                     padding = 'same')\n",
    "\n",
    "            tower3 = tf.layers.max_pooling2d(h, pool_size = (3, 3), strides = (1, 1),\n",
    "                                            padding = 'same')\n",
    "            tower3 = tf.layers.conv2d(tower3, filters = 32, kernel_size = (1, 1),\n",
    "                                     padding = 'same')\n",
    "\n",
    "            h = tf.concat([tower0, tower1, tower2, tower3], axis = -1)\n",
    "            h = tf.nn.relu(h)\n",
    "            if i % 2 == 0 and i != 0:\n",
    "                h = tf.layers.max_pooling2d(h, pool_size = (2, 2), strides = (1, 1), padding = 'same')\n",
    "        \n",
    "        Z0 = tf.layers.flatten(h)\n",
    "        Z0 = tf.layers.dense(Z0, units = 512, activation = 'relu')\n",
    "        Z0 = tf.layers.dropout(Z0, 0.2)\n",
    "        \n",
    "        Z0 = tf.layers.dense(Z0, units = 32)\n",
    "        Z_out = Z0\n",
    "\n",
    "        O = tf.layers.dense(Z0, units = 1)\n",
    "        \n",
    "        return O, Z_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function of L2 Loss\n",
    "def L2_Loss(y, y_hat):\n",
    "    return tf.reduce_mean((y - y_hat) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate data\n",
    "fake_data = generator(z, y)\n",
    "\n",
    "# Feed both real and fake data into the regressor for latent features\n",
    "true_pred, true_logit = regressor(real_data)\n",
    "fake_pred, fake_logit = regressor(fake_data, reuse = True)\n",
    "\n",
    "# Use latent feature, bandgap combination to perform authentication\n",
    "true_label_pred = discriminator(true_logit, y)\n",
    "fake_label_pred = discriminator(fake_logit, y, reuse = True)\n",
    "\n",
    "gen_sample = generator(z, y, reuse = True)\n",
    "gen_pred, gen_logit = regressor(gen_sample, reuse = True)\n",
    "\n",
    "# Regressor helper loss:\n",
    "R_loss = L2_Loss(true_pred, y)\n",
    "\n",
    "# Discriminator losses:\n",
    "D_real_loss = L2_Loss(true_label_pred, 0.9 * tf.ones_like(true_label_pred))\n",
    "D_fake_loss = L2_Loss(fake_label_pred, tf.zeros_like(fake_label_pred))\n",
    "D_loss = (D_real_loss + D_fake_loss)/2\n",
    "\n",
    "# Generator loss:\n",
    "G_loss = L2_Loss(fake_label_pred, 0.9 * tf.ones_like(fake_label_pred)) + 25 * L2_Loss(fake_pred, y)\n",
    "\n",
    "# Opimizer\n",
    "lr = 1e-4\n",
    "\n",
    "# Getting variables\n",
    "tvars = tf.trainable_variables()\n",
    "d_vars = [var for var in tvars if 'D' in var.name]\n",
    "g_vars = [var for var in tvars if 'G' in var.name]\n",
    "r_vars = [var for var in tvars if 'R' in var.name]\n",
    "\n",
    "# Setting up the optimizers\n",
    "opt_d = tf.train.AdamOptimizer(learning_rate = lr, beta1 = 0.5).minimize(D_loss, var_list = d_vars)\n",
    "opt_g = tf.train.AdamOptimizer(learning_rate = lr, beta1 = 0.5).minimize(G_loss, var_list = g_vars)\n",
    "opt_r = tf.train.AdamOptimizer(learning_rate= lr).minimize(R_loss, var_list = r_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_gan = 256 \n",
    "batch_size = 60\n",
    "\n",
    "batches = X_train.shape[0] // batch_size\n",
    "\n",
    "# Gather the losses for plot\n",
    "D_Losses = []\n",
    "G_Losses = []\n",
    "R_Losses = []\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "tol_time = 0\n",
    "\n",
    "# Training process\n",
    "with tf.Session(config=session_conf) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for e in range(epochs_gan+1):\n",
    "        a = time.time()\n",
    "        D_Losses_ = []\n",
    "        G_Losses_ = []\n",
    "        R_Losses_ = []\n",
    "\n",
    "        for b in range(batches):\n",
    "            idx = np.arange(b * batch_size, (b + 1) * batch_size)\n",
    "            train = X_train[idx,:,:,:]\n",
    "            batch_y = Y_train[idx,:]\n",
    "            batch_z = np.random.normal(0, 1, size = (batch_size, 128))\n",
    "            \n",
    "            # Each round, G, R and D will be trained using a batch of data\n",
    "            G_loss_, _ = sess.run([G_loss, opt_g], feed_dict = {y: batch_y, z: batch_z})\n",
    "            R_loss_, _= sess.run([R_loss, opt_r], feed_dict = {real_data: train, y: batch_y})\n",
    "            D_loss_, _ = sess.run([D_loss, opt_d], feed_dict = {real_data: train, y: batch_y, z: batch_z})\n",
    "            D_Losses_.append(D_loss_)\n",
    "            G_Losses_.append(G_loss_)\n",
    "            R_Losses_.append(R_loss_)\n",
    "        \n",
    "        # For timing purposes\n",
    "        b = time.time()\n",
    "        delta_t = b - a\n",
    "        tol_time += delta_t\n",
    "        \n",
    "        D_Losses.append(np.mean(D_Losses_))\n",
    "        G_Losses.append(np.mean(G_Losses_))\n",
    "        R_Losses.append(np.mean(R_Losses_))\n",
    "        \n",
    "        # Generate data and plot loss functions for sanity check every 10 epochs\n",
    "        if e % 10 == 0:\n",
    "            print('Currently on epoch {} of {}'.format(e, epochs_gan))\n",
    "            print('G Loss: {}, D Loss: {}, R Loss: {}'.format(np.mean(G_Losses_), np.mean(D_Losses_), np.mean(R_Losses_)))\n",
    "            samples = []\n",
    "            scores = []\n",
    "            sample_y = np.random.uniform(0.49, 2.15, size = (1, 1))\n",
    "            sample_y = np.round(sample_y, 4)\n",
    "            sample_y_ = sample_y * np.ones([100, 1])\n",
    "            sample_z = np.random.normal(0, 1, size = (100, 128))\n",
    "                \n",
    "            gen_sample_, pred_score = sess.run([gen_sample, gen_pred], feed_dict = {y: sample_y_, z: sample_z})\n",
    "\n",
    "            samples = np.array(gen_sample_)\n",
    "            samples = samples.reshape([100,4,4,1])\n",
    "            samples = np.where(samples < 0, -1, 1)\n",
    "            \n",
    "            S_ = samples.tolist()\n",
    "            not_in = 0\n",
    "            for s_ in S_:\n",
    "                if s_ not in X_train.tolist():\n",
    "                    not_in += 1\n",
    "            \n",
    "            print('Current number of unique samples: {}'.format(np.shape(np.unique(samples[:,:,:,0], axis = 0))[0]))\n",
    "            print('------------')\n",
    "            print('Currently Generating {}'.format(sample_y))\n",
    "            print('Predicted bandgap: {}'.format(np.mean(pred_score)))\n",
    "            print('Samples not in database :{}'.format(not_in))\n",
    "            print('------------')\n",
    "            \n",
    "            if e != 0:\n",
    "                plt.plot(G_Losses)\n",
    "                plt.plot(D_Losses)\n",
    "                plt.plot(R_Losses)\n",
    "                plt.legend(['G Loss', 'D Loss', 'R Loss'])\n",
    "                plt.show()\n",
    "        \n",
    "        # Save the model every 10 epochs\n",
    "        if e % 10 == 0:\n",
    "            print('-----------')\n",
    "            print('Saving current model')\n",
    "            saver.save(sess, \"cGAN_model.ckpt\")\n",
    "            with open('cGAN_Losses.pickle', 'wb') as f:\n",
    "                pickle.dump((G_Losses, D_Losses, R_Losses), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average time spent on training\n",
    "print('average running time: {}'.format(tol_time / 256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check on the accuracy of the regressor\n",
    "saver = tf.train.Saver()\n",
    "batch_size = 60\n",
    "\n",
    "batches = X_train.shape[0] // batch_size\n",
    "preds = []\n",
    "\n",
    "with tf.Session(config = session_conf) as sess:\n",
    "    saver.restore(sess, 'cGAN_model.ckpt')\n",
    "    for b in range(batches):\n",
    "        if b % 50 == 0:\n",
    "            print('Currently: {}/{}'.format(b, batches))\n",
    "        idx = np.arange(b * batch_size, (b + 1) * batch_size)\n",
    "        data = X_train[idx,:,:,:]\n",
    "        predicted, _ = sess.run(regressor(real_data, reuse = True), feed_dict = {real_data: data})\n",
    "        \n",
    "        preds.extend(predicted.reshape([-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fractional MAE for regressor\n",
    "preds = np.asarray(preds)\n",
    "print('Fractional MAE: {}'.format(np.mean(np.abs((preds - Y_train.reshape([-1])) / Y_train.reshape([-1])))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Generate new data for testing purposes\n",
    "\n",
    "OUT = pd.DataFrame()\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "for K in range(5 + 1):\n",
    "    print('Progress: {}/{}'.format(K, 5))\n",
    "    with tf.Session(config=session_conf) as sess:\n",
    "        saver.restore(sess, 'cGAN_model.ckpt')\n",
    "        print('------------')\n",
    "        Samples = []\n",
    "        labels = []\n",
    "        Inception_scores = []\n",
    "        gen_preds = []\n",
    "\n",
    "        for i in range(50):\n",
    "            labels_pred = []\n",
    "            sample_y = np.random.uniform(0.49, 2.15, size = (1, 1))\n",
    "            sample_y = np.round(sample_y, 4)\n",
    "            sample_y_ = sample_y * np.ones([100, 1])\n",
    "\n",
    "            sample_z = np.random.normal(0, 1, size = (100, 128))\n",
    "            gen_sample = sess.run(generator(z, y, reuse = True), feed_dict = {z: sample_z, y: sample_y_})\n",
    "\n",
    "            gen_sample = np.where(gen_sample < 0, -1, 1)\n",
    "            gen_score = sess.run(R_loss, feed_dict = {real_data: gen_sample, y: sample_y_})\n",
    "\n",
    "            Inception_scores.extend(list(np.unique(gen_score)))\n",
    "\n",
    "            gen_pred_,_ = sess.run(regressor(real_data, reuse = True), feed_dict = {real_data: np.unique(gen_sample, axis = 0)})\n",
    "            gen_preds.append(gen_pred_)\n",
    "\n",
    "            # print('------------')\n",
    "            gen_sample = np.array(gen_sample)\n",
    "            gen_sample = gen_sample.reshape([100,4,4,1])\n",
    "            Samples.extend(list(np.unique(gen_sample, axis = 0)))\n",
    "            labels.extend(list(sample_y * np.ones(shape = (1, np.shape(np.unique(gen_sample[:,:,:,0], axis = 0))[0]))))\n",
    "\n",
    "    labels_ = []\n",
    "    for l in labels:\n",
    "        for j in range(len(l)):\n",
    "            labels_.append(l[j])\n",
    "            \n",
    "    Gen_preds_ = []\n",
    "    for l in gen_preds:\n",
    "        for j in range(len(l)):\n",
    "            Gen_preds_.append(l[j])\n",
    "            \n",
    "    Samples_ = np.where(np.array(Samples)<0, 0, 1)\n",
    "    \n",
    "    summary = {}\n",
    "    summary['X'] = []\n",
    "    summary['require_label'] = []\n",
    "    summary['predicted_label'] = []\n",
    "\n",
    "    for s, l, gen_p in zip(Samples_, labels_, Gen_preds_):\n",
    "        summary['X'].append(int('0b'+''.join(list(s.reshape([16,]).astype('str').tolist())), 2))\n",
    "        summary['require_label'].append(l)\n",
    "        summary['predicted_label'].append(gen_p[0])\n",
    "        \n",
    "    out = pd.DataFrame(summary)\n",
    "    out = out.sort_values('X').reset_index().drop('index', axis = 1)\n",
    "\n",
    "    OUT = pd.concat([OUT, out], axis = 0, ignore_index = True)\n",
    "    \n",
    "    with open('summary.pickle', 'wb') as f:\n",
    "        pickle.dump(OUT, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fractional MAE for generation\n",
    "r = OUT.require_label.values\n",
    "p = OUT.predicted_label.values\n",
    "\n",
    "print('Fractional MAE: {}'.format(np.mean(np.abs((p - r) / r))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUT.to_csv('summary.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
